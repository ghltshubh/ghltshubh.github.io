---
---


@INPROCEEDINGS{Gahlot2024,
        abbr={NeurIPS},
        bibtex_show={true},
        author={Gahlot, Shubhankar},
        booktitle={2024 Neural Information Processing Systems (NeurIPS)}, 
        title={Eco-Drive Revolution: Reinforcement Learning-Enhanced Cruise Control for Fuel Efficiency and Climate Impact}, 
        year={2024},
        volume={},
        number={},
        pages={},
        abstract={Automobiles are one of the main contributors to climate change. We propose an enhanced Advanced Cruise Control (ACC) system that leverages Reinforcement Learning (RL) using location and terrain data along with onboard sensors and cameras to provide better driving characteristics and improve fuel economy.},
        keywords={},
        doi={},
        html = {https://drive.google.com/file/d/1zh5raTX8u1Ke74vzXxbQWSTAP46_-Ace/view},
        ISSN={},
        month={Dec},
        selected={true}
  }



@INPROCEEDINGS{8945109,
        abbr={IEEE/ACM DLS},
        bibtex_show={true},
        author={Yin, Junqi and Gahlot, Shubhankar and Laanait, Nouamane and Maheshwari, Ketan and Morrison, Jack and Dash, Sajal and Shankar, Mallikarjun},
        booktitle={2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS)}, 
        title={Strategies to Deploy and Scale Deep Learning on the Summit Supercomputer}, 
        year={2019},
        volume={},
        number={},
        pages={84-94},
        abstract={The rapid growth and wide applicability of Deep Learning (DL) frameworks poses challenges to computing centers which need to deploy and support the software, and also to domain scientists who have to keep up with the system environment and scale up scientific exploration through DL. We offer recommendations for deploying and scaling DL frameworks on the Summit supercomputer, currently atop the Top500 list, at the Oak Ridge National Laboratory Leadership Computing Facility (OLCF). We discuss DL software deployment in the form of containers, and compare performance of native-built frameworks and containerized deployment. Software containers show no noticeable negative performance impact and exhibit faster Python loading times and promise easier maintenance. To explore strategies for scaling up DL model training campaigns, we assess DL compute kernel performance, discuss and recommend I/O data formats and staging, and identify communication needs for scalable message exchange for DL runs at scale. We recommend that users take a step-wise tuning approach beginning with algorithmic kernel choice, node I/O configuration, and communications tuning as best-practice. We present baseline examples of scaling efficiency 87% for a DL run of ResNet50 running on 1024 nodes (6144 V100 GPUs).},
        keywords={},
        doi={10.1109/DLS49591.2019.00016},
        html = {https://ieeexplore.ieee.org/document/8945109},
        ISSN={},
        month={Nov},
        selected={true}
  }

@INPROCEEDINGS{9457561,
        abbr={CSCI},
        bibtex_show={true},
        author={Gahlot, Shubhankar and Yin, Junqi and Shankar, Mallikarjun Arjun},
        booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
        title={Data optimization for large batch distributed training of deep neural networks}, 
        year={2020},
        volume={},
        number={},
        pages={1197-1203},
        abstract={Distributed training in deep learning (DL) is common practice as data and models grow. The current practice for distributed training of deep neural networks faces the challenges of communication bottlenecks when operating at scale, and model accuracy deterioration with an increase in global batch size. Present solutions focus on improving message exchange efficiency as well as implementing techniques to tweak batch sizes and models in the training process. The loss of training accuracy typically happens because the loss function gets trapped in a local minima. We observe that the loss landscape minimization is shaped by both the model and training data and propose a data optimization approach that utilizes machine learning to implicitly smooth out the loss landscape resulting in fewer local minima. Our approach filters out data points which are less important to feature learning, enabling us to speed up the training of models on larger batch sizes to improved accuracy.},
        keywords={},
        doi={10.1109/CSCI51800.2020.00225},
        pdf = {https://www.osti.gov/servlets/purl/1807275},
        html = {https://www.osti.gov/biblio/1807275},
        ISSN={},
        month={Dec},
        selected={true}
        }

@INPROCEEDINGS{2021AGUFMIN23B..04A,
        abbr={AGU},
        bibtex_show={true},
        author = {{Acharya}, Ashish and {Davis}, Carson and {Koehl}, Derek and {Ramasubramanian}, Muthukumaran and {Gahlot}, Shubhankar and {Gurung}, Iksha and {Ramachandran}, Rahul},
        title = "{Verb Sense Disambiguation for Densifying Knowledge Graphs in Earth Science}",
        booktitle = {AGU Fall Meeting Abstracts},
        year = 2021,
        volume = {2021},
        month = dec,
          eid = {IN23B-04},
        pages = {IN23B-04},
        abstract = "{Knowledge graphs are graphical representations of knowledge using
                    entities and their relationships. Properly structured, they can
                    be a powerful way to surface latent relationships among well-
                    defined entities. By breaking down sentences into their semantic
                    components, an Earth science corpus can be represented as a
                    graph, with verbs acting as the relationship edges between
                    entity nodes. This allows domain information to be captured with
                    high precision. However, since there are multiple verbs in
                    English that can be used to denote the same meaning, this high
                    precision comes at the cost of sparsity of connections and query
                    results. In this presentation, we show a technique where we
                    disambiguate the meaning of a verb in a given sentence using
                    word2vec, with the aim to consolidate it into one among a
                    limited number of synonym sets. This leads to a denser graph and
                    more matches for a given query.}",
        adsurl = {https://ui.adsabs.harvard.edu/abs/2021AGUFMIN23B..04A},
        html = {https://ui.adsabs.harvard.edu/abs/2021AGUFMIN23B..04A},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System},
        selected={true}
}



@INPROCEEDINGS{2021AGUFMIN21A..05G,
        abbr={AGU},
        bibtex_show={true},
        author = {{Gurung}, Iksha and {Ramasubramanian}, Muthukumaran and {Gahlot}, Shubhankar and {Bollinger}, Andrew and {Maskey}, Manil and {Ramachandran}, Rahul},
        title = "{Machine Learning pipeline for Earth Science using Sagemaker}",
        booktitle = {AGU Fall Meeting Abstracts},
          year = 2021,
        volume = {2021},
        month = dec,
          eid = {IN21A-05},
        pages = {IN21A-05},
        abstract = "{Machine learning has risen to the forefront of solving various problems in scientific research. 
                    They differ from traditional problem
                    solving by modeling presented data using stochastic processes as
                    opposed to deterministic processes. This presents unique
                    challenges to overcome for successful implementation, namely
                    data parallelization and scalable computation. While machine
                    learning algorithms are being widely adopted across the
                    scientific community, setting up scalable data and computation
                    environments are increasingly becoming a barrier to overcome.
                    Modern cloud providers offer services that address these
                    challenges by automatically provisioning the environment,
                    enabling the scientists to focus solely on the algorithm
                    details. With this presentation, we show how SageMaker, a
                    service from AWS that aims to accelerate ML research, can be
                    used for an Earth Science use-case. In addition, We also
                    showcase ImageLabeler: A cloud native labeling tool for labeling
                    Earth Science Events. The tool simplifies importing of labeled
                    datasets into cloud environments such as Sagemaker.}",
        adsurl = {https://ui.adsabs.harvard.edu/abs/2021AGUFMIN21A..05G},
        html = {https://ui.adsabs.harvard.edu/abs/2021AGUFMIN21A..05G},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2022esoar.10511103G,
        abbr={AGU},
        bibtex_show={true},
        author = {{Gahlot}, Shubhankar and {Ramasubramanian}, Muthukumaran and {Gurung}, Iksha and {Hansch}, Ronny and {Molthan}, Andrew and {Maskey}, Manil},
        title = "{Curating flood extent data and leveraging citizen science for benchmarking machine learning solutions}",
        abstract = {We present a labeled machine learning (ML) training dataset derived from Sentinel 1 C-band synthetic aperture radar (SAR)
        data for flood events. In this paper, we detail the steps to collect, pre-process, label, curate, and catalog the training dataset.
        Development of benchmark ML models and usage of the training datasets for a data science competition are also presented.},
        journal = {ESS Open Archive eprints},
        keywords = {Earth Science},
         year = 2022,
        month = apr,
        volume = {105},
          eid = {essoar.10511103},
        pages = {essoar.10511103},
          doi = {10.1002/essoar.10511103.1},
        adsurl = {https://ui.adsabs.harvard.edu/abs/2022esoar.10511103G},
        html = {https://ui.adsabs.harvard.edu/abs/2022esoar.10511103G},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System},
        dataset = {https://doi.org/10.24432/C50P62},
        selected={true}
}

@INPROCEEDINGS{2021AGUFM.A25J1812P,
        abbr={AGU},
        bibtex_show={true},
        author = {{Priftis}, George and {Kaulfus}, Aaron and {Ramasubramanian}, Muthukumaran and {Gahlot}, Shubhankar and {Khatri}, Manisha and {Gurung}, Iksha and {Maskey}, Manil and {Ramachandran}, Rahul and {Christopher}, Sundar},
        title = "{A Novel Machine Learning Method for Surface PM2.5 Estimations from Geostationary Satellites}",
        booktitle = {AGU Fall Meeting Abstracts},
         year = 2021,
        volume = {2021},
        month = dec,
          eid = {A25J-1812},
        pages = {A25J-1812},
        abstract = "{Particulate matter (PM) with a diameter of less or equal to 2.5 m, known as PM2.5, affects human health as it penetrates the respiratory
        system. The Environmental Protection Agency (EPA) measures the
        atmospheric concentration of PM2.5 using air quality monitors
        stationed throughout the Continental United States (CONUS). Such
        measurements are points on a spatial domain and therefore, might
        not be representative of the air quality at nearby areas
        considering that the composition of the atmosphere is highly
        variable from place to place. Satellite based AOD permits a
        spatially uniform means of estimating PM2.5 and new
        geostationary satellites provide high temporal and spatial
        resolution estimation of AOD. However, the concentration of
        PM2.5 is non-linearly dependent on other atmospheric parameters
        that include relative humidity, temperature, and height of the
        planetary boundary layer. This information may be estimated at
        similar spatial and temporal resolutions as AOD from numerical
        modeling such as from the National Oceanic and Atmospheric
        Administrations (NOAA) High Resolution Rapid Refresh (HRRR)
        model which resolves near real-time atmospheric conditions over
        the CONUS. The estimation of PM2.5 concentration is a multi-
        parametric problem that considers the effect of temporal
        dependencies among the different parameters. Deep learning
        approaches are appropriate for such complex estimation problems
        as they intrinsically capture relations among multiple non-
        linear parameters. This study compares deep-learning methods to
        traditional regression analysis to demonstrate the capabilities
        of these methods in predicting PM2.5 concentrations.
        Additionally, a novel ensemble learning approach is employed to
        identify scientific processes that could further improve the
        estimation of PM2.5 concentration. Utilizing Long Short-Term
        Memory (LSTM) neural networks, which are suitable for
        multivariate time series estimation problems as they are capable
        of learning long-term dependencies, individual models are
        created for each EPA station and trained on the aforementioned
        dataset collocated over each station. Individual station models
        are merged if the model's performance is improved by reducing
        the root mean squared error (RMSE) metric. This ensemble
        training method ultimately reduces the RMSE value. Evaluation of
        these results provide insights into physical processes and
        related observable parameters that may contribute to PM2.5
        concentrations. Identified parameters evaluated to be
        statistically different between the merged and unmerged models
        are expected to improve overall performance. These new
        parameters are then utilized for reevaluation of the deep
        learning methods with an extreme gradient boosting model with an
        RMSE of 5.5 providing the best results.}",
        adsurl = {https://ui.adsabs.harvard.edu/abs/2021AGUFM.A25J1812P},
        html = {https://ui.adsabs.harvard.edu/abs/2021AGUFM.A25J1812P},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{2021AGUFMIN33A..04G,
        abbr={AGU},
        bibtex_show={true},   
        author = {{Gahlot}, Shubhankar and {Ramasubramanian}, Muthukumaran and {Gurung}, Iksha and {Freitag}, Brian and {Alemohammad}, Hamed and {Maskey}, Manil and {Ramachandran}, Rahul},
        title = "{Leveraging citizen science and Artificial intelligence for monitoring and estimating hazardous events}",
        booktitle = {AGU Fall Meeting Abstracts},
          year = 2021,
        volume = {2021},
        month = dec,
          eid = {IN33A-04},
        pages = {IN33A-04},
        abstract = "{Floods and hurricanes are few of the major natural disasters that cause
        immense damage to property and lives every year. Therefore,
        knowing the true extent of these disasters is crucial for
        emergency management and resource allocation not only by federal
        agencies like the Federal Emergency Management Agency (FEMA) but
        also by local authorities and nonprofits. Monitoring these
        events in-situ is difficult as it is hazardous to operate in a
        disaster zone. Remote sensing, in conjunction with machine
        learning, has been used extensively in the community to monitor
        these events. Finding a machine learning solution is an
        exhaustive process. Citizen science has been used extensively to
        find the best solution for problems in both scientific and
        commercial sectors. As part of incorporating citizen science for
        detecting and estimating extents of natural disasters, we hosted
        competitions to involve the broader science community to
        estimate the hurricane wind speeds and flood extents based on
        satellite images. In this presentation, we will discuss the
        methods used to generate the datasets, results from the
        competition, and the lessons learned.}",
        adsurl = {https://ui.adsabs.harvard.edu/abs/2021AGUFMIN33A..04G},
        html = {https://ui.adsabs.harvard.edu/abs/2021AGUFMIN33A..04G},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{LINC2019:Changing_state_of_literacy,
        abbr={LINC},
        bibtex_show={true},
        author    = {Aanandita Gahlot and Shubhankar Gahlot},
        title     = {Changing the state of literacy in the Digital Age in India},
        abstract  = {India as an emerging economy deals with troubles in literacy due to factors like shortage of quality academic institutions and unsuitable curriculum. Digital Technology is accredited as something which can bridge the gap between quality institutions and individuals and make learning more engaging.
        Indian Government has made use of technology in the best possible way and launched Pradhan Mantri Gramin Digital Saksharta Abhiyan (PMGDISHA)‡ under its Digital India initiative. It has been initiated to make at least one individual from each household digitally literate so that they develop the skills which will be needed to link with the rapidly growing digital world. This scheme aims to target the rural population including the disparaged sections of society like minorities, Below Poverty Line (BPL), women and differently-abled people.
        The use of technology in education has transmuted the whole system of education. This paper is aimed at exploring the changing state of literacy in India after introducing PMGDISHA.},
        booktitle = {Proceedings of the MIT LINC 2019 Conference},
        editor    = {Claudia Urrea},
        series    = {EPiC Series in Education Science},
        volume    = {3},
        pages     = {98--107},
        year      = {2020},
        publisher = {EasyChair},
        bibsource = {EasyChair, https://easychair.org},
        issn      = {2516-2306},
        url       = {https://easychair.org/publications/paper/H18c},
        pdf       = {https://easychair.org/publications/open/H18c},
        doi       = {10.29007/qbpr}
  }

